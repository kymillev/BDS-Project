{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import urllib.parse\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets were collected with center point in several cities and a varying radius, dependant on the size of the country\n",
    "\n",
    "### See file: get_tweets_countries.py\n",
    "\n",
    "### Now we merge all of the tweets per country and save it with the respected ISO-3 codes, needed to later get the correct geojson polygons for each country, to visualize on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Australia', 'Austria', 'Belgium', 'China', 'Denmark', 'Egypt', 'Finland', 'Germany', 'Hungary', 'India', 'Iran', 'Italy', 'New Zealand', 'Nigeria', 'Pakistan', 'Poland', 'Romania', 'Spain', 'Sweden', 'The Netherlands', 'The United Kingdom', 'The United States of America', 'Ukraine']\n"
     ]
    }
   ],
   "source": [
    "countries = {'BEL':{'cities':['Antwerp','Brussels','Liege','Mons'],'name':'Belgium'},\n",
    "            'NLD':{'cities':['Amsterdam','Eindhoven','Groningen','Rotterdam','Utrecht','Zwolle'],'name':'The Netherlands'},\n",
    "            'ESP':{'cities':['Barcelona','Bilbao','Madrid','Sevilla','Valencia','Zaragoza'],'name':'Spain'},\n",
    "            'USA':{'cities':['Atlanta','Austin','Chicago','NewYork','Sanfrancisco','Seattle'],'name':'The United States of America'},\n",
    "            'DEU':{'cities':['Berlin','Frankfurt','Hamburg','Munich'],'name':'Germany'},\n",
    "            'CHN':{'cities':['Beijing','Wuhan'],'name':'China'},\n",
    "            'ITA':{'cities':['Milan','Rome'],'name':'Italy'},\n",
    "            'GBR':{'cities':['Birmingham','Glasgow','Leeds','London','Manchester'],'name':'The United Kingdom'},\n",
    "            'IND':{'cities':['Delhi','Mumbai'],'name':'India'},\n",
    "            'SWE':{'cities':['Stockholm'],'name':'Sweden'},\n",
    "            'FIN':{'cities':['Helsinki'],'name':'Finland'},\n",
    "            'DNK':{'cities':['Copenhagen'],'name':'Denmark'},\n",
    "            'PAK':{'cities':['Karachi','Islamabad'],'name':'Pakistan'},\n",
    "            'AUS':{'cities':['Perth','Sydney'],'name':'Australia'},\n",
    "            'ROU':{'cities':['Bucharest'],'name':'Romania'},\n",
    "            'HUN':{'cities':['Budapest'],'name':'Hungary'},\n",
    "            'EGY':{'cities':['Cairo'],'name':'Egypt'},\n",
    "            'UKR':{'cities':['Kiev'],'name':'Ukraine'},\n",
    "            'IRN':{'cities':['Tehran'],'name':'Iran'},\n",
    "            'AUT':{'cities':['Vienna'],'name':'Austria'},\n",
    "            'POL':{'cities':['Warsaw'],'name':'Poland'},\n",
    "            'NZL':{'cities':['Auckland'],'name':'New Zealand'},\n",
    "            'NGA':{'cities':['Kano'],'name':'Nigeria'}\n",
    "            }\n",
    "\n",
    "cities = set()\n",
    "country_names = []\n",
    "for k,v in countries.items():\n",
    "    cities |= set(v['cities'])\n",
    "    country_names.append(v['name'])\n",
    "print(sorted(country_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_AUS\n",
      "s_AUT\n",
      "s_BEL\n",
      "s_CHN\n",
      "s_DEU\n",
      "s_DNK\n",
      "s_EGY\n",
      "s_ESP\n",
      "s_FIN\n",
      "s_GBR\n",
      "s_HUN\n",
      "s_IND\n",
      "s_IRN\n",
      "s_ITA\n",
      "s_NGA\n",
      "s_NLD\n",
      "s_NZL\n",
      "s_PAK\n",
      "s_POL\n",
      "s_ROU\n",
      "s_SWE\n",
      "s_UKR\n",
      "s_USA\n",
      "Total tweets: 56532\n"
     ]
    }
   ],
   "source": [
    "# Checks if we have manually put in all of the cities, should print nothing\n",
    "n_total = 0\n",
    "for f in os.listdir('tweets_new'):\n",
    "    if f.endswith('.json'):\n",
    "        city = f.split('tweet')[-1].replace('.json','')\n",
    "        tweets = json.load(open('tweets_new/'+f,'r'))['tweets']\n",
    "        n_total += len(tweets)\n",
    "        if city not in cities:\n",
    "            print(city)\n",
    "print('Total tweets:',n_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocode tweets based on user location\n",
    "\n",
    "### Almost none of the tweets have coordinates attached, so we use geocoders and fuzzy matching to get them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_request(tweet):    \n",
    "    if tweet['coordinates'] is not None:\n",
    "        return tweet['coordinates']\n",
    "    elif tweet['place_coords'] is not None:\n",
    "        return np.array(tweet['place_coords'][0]).mean(axis=0).tolist()\n",
    "    \n",
    "    # Order: Geonames => TomTom => Google Maps \n",
    "    # If we get matches, return the best based on string similarity\n",
    "    scores = []\n",
    "    query = tweet['user_location'].lower()\n",
    "    if len(query) <= 3:\n",
    "        return None\n",
    "    \n",
    "    for geocoder in [geonames_request, tomtom_request, google_geocode_request]:\n",
    "        result = geocoder(query)\n",
    "        \n",
    "        for r in result:\n",
    "            name = r['name']\n",
    "            try:\n",
    "                scores.append(fuzz.partial_ratio(query, name.lower()))\n",
    "            # When user location is not in our alfabet (e.g. russian), this will give an error, so we return None\n",
    "            except ValueError:\n",
    "                return None\n",
    "        \n",
    "        if len(result) > 0:\n",
    "            result = result[np.argmax(scores)]\n",
    "            return [result['lng'],result['lat']]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# This code was reused from another project\n",
    "def geonames_request(query, lat=None, lng=None, radius=5000, max_rows=40):\n",
    "    usernames = ['XXX', 'XXX', 'XXX']\n",
    "    idx = random.randint(0, 2)\n",
    "    username = usernames[idx]\n",
    "    query = urllib.parse.quote(query, safe='')\n",
    "    url = 'http://api.geonames.org/searchJSON?q=' + query + '&lang=local&&country=BE&fuzzy=0.3&maxRows=' + str(max_rows) + '&username=' + username\n",
    "    if radius is not None:\n",
    "        radius = '&radius=' + str(radius / 1000)\n",
    "        url += radius\n",
    "    if lat is not None and lng is not None:\n",
    "        se, nw = get_bbox_from_radius(lat, lng, radius / 1000)\n",
    "        s = str(se[0])\n",
    "        e = str(se[1])\n",
    "        n = str(nw[0])\n",
    "        w = str(nw[1])\n",
    "        bounds = '&south=' + s + '&north=' + n + '&west=' + w + '&east=' + e\n",
    "        url += bounds\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    geonames_result = response.json()\n",
    "\n",
    "    result = []\n",
    "\n",
    "    duplicate = False\n",
    "    unique = {}\n",
    "\n",
    "    for i, g in enumerate(geonames_result['geonames']):\n",
    "\n",
    "        obj = {'lat': '', 'lng': '', 'name': '', 'type': ''}\n",
    "        type_ = ''\n",
    "        name = ''\n",
    "\n",
    "        if 'lat' in g:\n",
    "            obj['lat'] = float(g['lat'])\n",
    "        if 'lng' in g:\n",
    "            obj['lng'] = float(g['lng'])\n",
    "        # Prefer to use name for local\n",
    "        if 'name' in g:\n",
    "            name = g['name']\n",
    "            obj['name'] = name\n",
    "        if 'fcl' in g:\n",
    "            fcl = g['fcl']\n",
    "            if fcl == 'P':\n",
    "                type_ = 'locality'\n",
    "            elif fcl == 'A':\n",
    "                type_ = 'administrative_area'\n",
    "            elif fcl == 'H':\n",
    "                type_ = 'stream,lake'\n",
    "            elif fcl == 'R':\n",
    "                type_ = 'road'\n",
    "            elif fcl == 'L':\n",
    "                type_ = 'parks,area'\n",
    "            elif fcl == 'V':\n",
    "                type_ = 'forest'\n",
    "            elif fcl == 'T':\n",
    "                type_ = 'mountain,hill'\n",
    "            elif fcl == 'S':\n",
    "                type_ = 'spot,building,farm'\n",
    "            else:\n",
    "                type_ = fcl\n",
    "\n",
    "            obj['type'] = type_\n",
    "\n",
    "        if name in unique:\n",
    "            \n",
    "            idx = unique[name]\n",
    "            duplicate = result[idx]\n",
    "\n",
    "            if duplicate['type'] == 'administrative_area' and type_ == 'locality':\n",
    "                result[idx] = obj\n",
    "            '''\n",
    "            elif duplicate['type'] == 'locality' and type_ == 'administrative_area':\n",
    "                continue\n",
    "            '''\n",
    "        else:\n",
    "            unique[obj['name']] = i\n",
    "\n",
    "        result.append(obj)\n",
    "\n",
    "    return result\n",
    "\n",
    "# This code was reused from another project\n",
    "def tomtom_request(query, lat=None, lng=None, radius=5000, idx_set=None):\n",
    "    \"\"\"\n",
    "    Fuzzy geocode query in a specified area\n",
    "    :param query:\n",
    "    :param idx_set:\n",
    "    :param lat:\n",
    "    :param lng:\n",
    "    :param radius:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    query = urllib.parse.quote(query, safe='')\n",
    "\n",
    "    url = 'https://api.tomtom.com/search/2/search/' + query + \\\n",
    "          '.json?typeahead=true&countrySet=BE&limit=100&minFuzzyLevel=1&maxFuzzyLevel=4&language=nl-BE&key' \\\n",
    "          '=XXX'\n",
    "\n",
    "    if idx_set is None:\n",
    "        idx_set = 'Geo,Addr,PAD,Str'\n",
    "\n",
    "    url += '&idxSet=' + idx_set\n",
    "\n",
    "    if lat is not None and lng is not None:\n",
    "        url += '&lat=' + str(lat) + '&lon=' + str(lng)\n",
    "    if radius is not None:\n",
    "        url += '&radius=' + str(radius)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        tomtom_result = response.json()\n",
    "    except BaseException as e:\n",
    "        print('ERROR TOMTOM REQUEST')\n",
    "        print('url', url)\n",
    "        print(response)\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    res = []\n",
    "    results = tomtom_result['results']\n",
    "\n",
    "    def lower_first_letter(s):\n",
    "        return s[:1].lower() + s[1:] if s else ''\n",
    "\n",
    "    for r in results:\n",
    "        obj = {}\n",
    "        type_ = r['type']\n",
    "        addr = r['address']\n",
    "        p = r['position']\n",
    "\n",
    "        if type_ == 'Geography':\n",
    "            entity_type = lower_first_letter(r['entityType'])\n",
    "            if entity_type in addr:\n",
    "                name = addr[entity_type]\n",
    "            elif 'municipalitySubdivision' in addr:\n",
    "                name = addr['municipalitySubdivision']\n",
    "            else:\n",
    "                name = addr['freeformAddress']\n",
    "                print('freeformaddres')\n",
    "\n",
    "        elif type_ == 'Street' or type_ == 'Point Address' or type_ == 'Address Range':\n",
    "            name = addr['streetName']\n",
    "\n",
    "        else:\n",
    "            print('\\nTYPEEE', type_, '\\n')\n",
    "            name = 'Unknown Type - TomTom'\n",
    "            print(r)\n",
    "\n",
    "        if 'dist' in r:\n",
    "            obj['dist'] = r['dist']\n",
    "\n",
    "        obj['lat'] = float(p['lat'])\n",
    "        obj['lng'] = float(p['lon'])\n",
    "        obj['name'] = name\n",
    "        obj['type'] = type_\n",
    "\n",
    "        res.append(obj)\n",
    "\n",
    "    return res\n",
    "\n",
    "# This code was reused from another project\n",
    "def google_geocode_request(query, lat=None, lng=None, radius=5000):\n",
    "    \"\"\"\n",
    "    Fuzzy geocoding query in a specified area\n",
    "    :param query:\n",
    "    :param lat:\n",
    "    :param lng:\n",
    "    :param radius:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    query = urllib.parse.quote(query, safe='')\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?address=' + query + \\\n",
    "          '&language=nl&region=be&key=XXX'\n",
    "\n",
    "    if lat is not None and lng is not None:\n",
    "        se, nw = get_bbox_from_radius(lat, lng, radius / 1000)\n",
    "        bounds = str(se[0]) + ',' + str(se[1]) + '|' + str(nw[0]) + ',' + str(nw[1])\n",
    "        url += '&bounds=' + bounds\n",
    "\n",
    "    response = requests.get(url)\n",
    "    results = []\n",
    "    google_result = response.json()\n",
    "\n",
    "    def check_int(n):\n",
    "        try:\n",
    "            n = int(n)\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            return False\n",
    "\n",
    "    for r in google_result['results']:\n",
    "        addr = r['address_components']\n",
    "        name = ''\n",
    "        type_ = ''\n",
    "        for a in addr:\n",
    "\n",
    "            # ignore house numbers as they are listed first\n",
    "            if a['types'] == ['street_number'] or check_int(a['long_name']):\n",
    "                continue\n",
    "\n",
    "            name = a['long_name']\n",
    "            type_ = ','.join(a['types'])\n",
    "            break\n",
    "\n",
    "        loc = r['geometry']['location']\n",
    "\n",
    "        obj = {'lat': loc['lat'], 'lng': loc['lng'], 'name': name, 'type': type_}\n",
    "        results.append(obj)\n",
    "\n",
    "    filtered = []\n",
    "    if lat is not None and lng is not None:\n",
    "        for r in results:\n",
    "            lat = r['lat']\n",
    "            lng = r['lng']\n",
    "\n",
    "            if lat < se[0] or lat > nw[0] or lng < nw[1] or lng > se[1]:\n",
    "                pass\n",
    "            else:\n",
    "                filtered.append(r)\n",
    "\n",
    "        return filtered\n",
    "    else:\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate per country and perform sentiment analysis + text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_geojson = json.load(open('countries.geojson','r'))\n",
    "try:\n",
    "    geocode_dict = json.load(open('geocode_dict.json','r'))\n",
    "except FileNotFoundError:\n",
    "    geocode_dict = {}\n",
    "    \n",
    "# lat,lng pairs    \n",
    "city_coordinates = {\n",
    "    'Madrid':[40.4168,-3.7038],\n",
    "    'Barcelona':[41.398371,2.1741],\n",
    "    'Sevilla':[37.382826,-5.973167],\n",
    "    'Zaragoza':[41.64531,-0.884861],\n",
    "    'Bilbao':[43.260919,-2.938764],\n",
    "    'Valencia':[39.466667,-0.366667],\n",
    "    'Brussels':[50.833333,4.33333],\n",
    "    'Antwerp':[51.213886,4.401514],\n",
    "    'Liege':[50.638674,5.570228],\n",
    "    'Mons':[50.45527,3.951623],\n",
    "    'Rome':[41.9,12.48333],\n",
    "    'Milan':[45.466667,9.2],\n",
    "    'Amsterdam':[52.35,4.916667],\n",
    "    'Rotterdam':[51.916667,4.5],\n",
    "    'Utrecht':[52.093813,5.119095],\n",
    "    'Eindhoven':[51.45,5.466667],\n",
    "    'Groningen':[53.216667,6.55],\n",
    "    'Zwolle':[52.505751,6.085822],\n",
    "    'London':[51.514248,-0.093145],\n",
    "    'Birmingham':[52.466667,-1.9166667],\n",
    "    'Manchester':[53.5,-2.216667],\n",
    "    'Leeds':[53.8,-1.583333],\n",
    "    'Glasgow':[55.833333,-4.25],\n",
    "    'Atlanta':[33.753746,-84.386330],\n",
    "    'Austin':[30.266666,-97.733330],\n",
    "    'NewYork':[40.730610,-73.935242],\n",
    "    'Chicago':[41.881832,-87.623177],\n",
    "    'Sanfrancisco':[37.773972,-122.431297],\n",
    "    'Seattle':[47.608013,-122.335167],\n",
    "    'Vienna':[48.2,16.366667],\n",
    "    'Copenhagen':[55.666667,12.583333],\n",
    "    'Stockholm':[59.333333,18.05],\n",
    "    'Berlin':[52.516667,13.4],\n",
    "    'Frankfurt':[50.11552,8.684167],\n",
    "    'Hamburg':[53.575323,10.01534],\n",
    "    'Munich':[48.15,11.583333],\n",
    "    'Budapest':[47.5,19.083333],\n",
    "    'Warsaw':[52.25,21.0],\n",
    "    'Kiev':[50.433333,30.516667],\n",
    "    'Bucharest':[44.433333,26.1],\n",
    "    'Helsinki':[60.175556,24.934167],\n",
    "    'Karachi':[24.9056,67.0822],\n",
    "    'Islamabad':[33.69,73.0551],\n",
    "    'Delhi':[28.651952,77.231495],\n",
    "    'Mumbai':[18.987807,72.836447],\n",
    "    'Beijing':[39.928819,116.388869],\n",
    "    'Wuhan':[30.583333,114.266667],\n",
    "    'Cairo':[30.07708,31.285909],\n",
    "    'Kano':[12.002381,8.51316],\n",
    "    'Tehran':[35.705,51.4216],\n",
    "    'Sydney':[-33.861481,151.205475],\n",
    "    'Perth':[-31.95224,115.861397],\n",
    "    'Auckland':[-36.866667,174.766667]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: BEL\n",
      "city Antwerp\n",
      "city Brussels\n",
      "city Liege\n",
      "city Mons\n",
      "Time elapsed: 0.768923282623291\n",
      "Total unique locations: 236\n",
      "\n",
      "Current: NLD\n",
      "city Amsterdam\n",
      "city Eindhoven\n",
      "city Groningen\n",
      "city Rotterdam\n",
      "city Utrecht\n",
      "city Zwolle\n",
      "Time elapsed: 1.5333583354949951\n",
      "Total unique locations: 556\n",
      "\n",
      "Current: ESP\n",
      "city Barcelona\n",
      "city Bilbao\n",
      "city Madrid\n",
      "city Sevilla\n",
      "city Valencia\n",
      "city Zaragoza\n",
      "Time elapsed: 1.0839056968688965\n",
      "Total unique locations: 928\n",
      "\n",
      "Current: USA\n",
      "city Atlanta\n",
      "city Austin\n",
      "city Chicago\n",
      "city NewYork\n",
      "city Sanfrancisco\n",
      "city Seattle\n",
      "Time elapsed: 3.4042372703552246\n",
      "Total unique locations: 2161\n",
      "\n",
      "Current: DEU\n",
      "city Berlin\n",
      "city Frankfurt\n",
      "city Hamburg\n",
      "city Munich\n",
      "Time elapsed: 1.2055394649505615\n",
      "Total unique locations: 2370\n",
      "\n",
      "Current: CHN\n",
      "city Beijing\n",
      "city Wuhan\n",
      "Time elapsed: 0.27649521827697754\n",
      "Total unique locations: 2408\n",
      "\n",
      "Current: ITA\n",
      "city Milan\n",
      "city Rome\n",
      "Time elapsed: 0.3682861328125\n",
      "Total unique locations: 2506\n",
      "\n",
      "Current: GBR\n",
      "city Birmingham\n",
      "city Glasgow\n",
      "city Leeds\n",
      "city London\n",
      "city Manchester\n",
      "Time elapsed: 4.1047608852386475\n",
      "Total unique locations: 3630\n",
      "\n",
      "Current: IND\n",
      "city Delhi\n",
      "city Mumbai\n",
      "Time elapsed: 1.685617208480835\n",
      "Total unique locations: 3919\n",
      "\n",
      "Current: SWE\n",
      "city Stockholm\n",
      "Time elapsed: 0.5338690280914307\n",
      "Total unique locations: 3968\n",
      "\n",
      "Current: FIN\n",
      "city Helsinki\n",
      "Time elapsed: 0.3767681121826172\n",
      "Total unique locations: 4003\n",
      "\n",
      "Current: DNK\n",
      "city Copenhagen\n",
      "Time elapsed: 0.3679928779602051\n",
      "Total unique locations: 4069\n",
      "\n",
      "Current: PAK\n",
      "city Karachi\n",
      "city Islamabad\n",
      "Time elapsed: 1.9046351909637451\n",
      "Total unique locations: 4343\n",
      "\n",
      "Current: AUS\n",
      "city Perth\n",
      "city Sydney\n",
      "Time elapsed: 1.0035576820373535\n",
      "Total unique locations: 4535\n",
      "\n",
      "Current: ROU\n",
      "city Bucharest\n",
      "Time elapsed: 0.1098630428314209\n",
      "Total unique locations: 4557\n",
      "\n",
      "Current: HUN\n",
      "city Budapest\n",
      "Time elapsed: 0.13570261001586914\n",
      "Total unique locations: 4585\n",
      "\n",
      "Current: EGY\n",
      "city Cairo\n",
      "Time elapsed: 0.4200103282928467\n",
      "Total unique locations: 4690\n",
      "\n",
      "Current: UKR\n",
      "city Kiev\n",
      "Time elapsed: 0.040225982666015625\n",
      "Total unique locations: 4702\n",
      "\n",
      "Current: IRN\n",
      "city Tehran\n",
      "Time elapsed: 0.18839573860168457\n",
      "Total unique locations: 4734\n",
      "\n",
      "Current: AUT\n",
      "city Vienna\n",
      "Time elapsed: 0.2774996757507324\n",
      "Total unique locations: 4780\n",
      "\n",
      "Current: POL\n",
      "city Warsaw\n",
      "Time elapsed: 0.10833597183227539\n",
      "Total unique locations: 4795\n",
      "\n",
      "Current: NZL\n",
      "city Auckland\n",
      "Time elapsed: 0.43436646461486816\n",
      "Total unique locations: 4873\n",
      "\n",
      "Current: NGA\n",
      "city Kano\n",
      "Time elapsed: 0.902756929397583\n",
      "Total unique locations: 5038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#source: https://github.com/trinker/lexicon/issues/2\n",
    "pronouns = {\"he\", \"her\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"i\", \n",
    "\"it\", \"its\", \"me\", \"mine\", \"my\", \"myself\", \"our\", \"ours\", \"ourselves\", \n",
    "\"she\", \"thee\", \"their\", \"them\", \"themselves\", \"they\", \"thou\", \n",
    "\"thy\", \"thyself\", \"us\", \"we\", \"ye\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "\"we\"}\n",
    "# some additional 'junk' words we want to avoid in the wordclouds\n",
    "other = {'amp','like','also','day','year','month','even','want','would','one','two','three','four','five','really',\n",
    "        'get','let','ever'}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ignored_words = stop_words | pronouns | other\n",
    "\n",
    "unique_loc = set()\n",
    "\n",
    "for country,obj in countries.items():\n",
    "    print('Current:',country)\n",
    "    merged = []\n",
    "    words = {}\n",
    "    avg_polarity = 0\n",
    "    n = 0\n",
    "    geojson = None\n",
    "    # get correct geojson\n",
    "    for f in countries_geojson['features']:\n",
    "        if f['properties']['ISO_A3'] == country:\n",
    "            geojson = f\n",
    "            break\n",
    "    assert geojson is not None,'NO GEOJSON FOUND FOR '+country\n",
    "        \n",
    "    t1 = time.time()\n",
    "    for city in obj['cities']:\n",
    "        print('city',city)\n",
    "        fname = 'tweets_new/tweet'+city+'.json'\n",
    "        tweets = json.load(open(fname,'r'))['tweets']\n",
    "        for tweet in tweets:\n",
    "            # get geolocation, save results to speed up duplicate locations\n",
    "            loc = tweet['user_location'].lower()\n",
    "            if len(loc) > 3:\n",
    "                if loc in geocode_dict:\n",
    "                    coordinates = geocode_dict[loc]\n",
    "                else:\n",
    "                    coordinates = geocode_request(tweet)\n",
    "                    geocode_dict[loc] = coordinates\n",
    "                    if coordinates is None:\n",
    "                        # change to lng,lat\n",
    "                        coordinates = city_coordinates[city][::-1]\n",
    "\n",
    "                    # save after each api request, so when we have an error it is still saved\n",
    "                    json.dump(geocode_dict,open('geocode_dict.json','w'))\n",
    "            else:\n",
    "                coordinates = None\n",
    "                \n",
    "            # preprocessing\n",
    "            processed_text = re.sub(pattern, ' ', tweet['text'].lower())\n",
    "            processed_text = processed_text.replace('  ',' ').strip()\n",
    "        \n",
    "            # sentiment\n",
    "            polarity = TextBlob(processed_text).sentiment.polarity\n",
    "            # remove stopwords and lemmatize\n",
    "            tokens = word_tokenize(processed_text) \n",
    "            tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            tokens = [t for t in tokens if t not in ignored_words] \n",
    "            tokens = [t for t in tokens if len(t) > 2 and not t.isnumeric()]\n",
    "            processed_text = ' '.join(t for t in tokens)\n",
    "            for t in tokens:\n",
    "                if t in words:\n",
    "                    words[t] += 1\n",
    "                else:\n",
    "                    words[t] = 1\n",
    "            \n",
    "            tweet['polarity'] = polarity\n",
    "            tweet['city_name'] = city\n",
    "            tweet['processed_text'] = processed_text\n",
    "            tweet['coordinates'] = coordinates\n",
    "            tweet['country'] = country\n",
    "            unique_loc.add(tweet['user_location'].lower())\n",
    "            avg_polarity += polarity\n",
    "            n += 1\n",
    "         \n",
    "        merged.extend(tweets)\n",
    "    print('Time elapsed:',time.time()-t1)\n",
    "    print('Total unique locations:',len(unique_loc))\n",
    "    print()\n",
    "    avg_polarity /= n\n",
    "    # Get top 100 words\n",
    "    top_words = dict(sorted(words.items(),reverse=True,key=lambda x:x[1])[:100])\n",
    "    #print('Top words:',top_words)\n",
    "    json.dump({'words':top_words,'polarity':avg_polarity,'tweets':merged,'geojson':geojson},open('tweets_new/tweets_'+country+'.json','w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the other dataset of tweets, we determine the country by checking if the coordinates are inside of any of the country polygons\n",
    "\n",
    "### For each tweet, perform the same sentiment analysis and preprocessing, check in which of the selected countries it was made, if any, and add the country name to the object. This will ensure both datasets are uniform, making further processing much easier\n",
    "\n",
    "#### Note: This was also possible to do more efficiently after importing the tweets into the database and by using the django GEOS operations, however, this way it was much easier to code and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BEL', 'NLD', 'ESP', 'USA', 'DEU', 'CHN', 'ITA', 'GBR', 'IND', 'SWE', 'FIN', 'DNK', 'PAK', 'AUS', 'ROU', 'HUN', 'EGY', 'UKR', 'IRN', 'AUT', 'POL', 'NZL', 'NGA']\n",
      "Total tweets: 41367\n",
      "Finished 5000 / 41367 time elapsed: 93.266 s\n",
      "Finished 10000 / 41367 time elapsed: 186.814 s\n",
      "Finished 15000 / 41367 time elapsed: 280.111 s\n",
      "Finished 20000 / 41367 time elapsed: 373.772 s\n",
      "Finished 25000 / 41367 time elapsed: 467.022 s\n",
      "Finished 30000 / 41367 time elapsed: 561.505 s\n",
      "Finished 35000 / 41367 time elapsed: 656.008 s\n",
      "Finished 40000 / 41367 time elapsed: 751.309 s\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import shape,Point \n",
    "\n",
    "shapes = []\n",
    "country_names = list(countries.keys())\n",
    "print(country_names)\n",
    "\n",
    "# Get shape of all the countries we have selected\n",
    "for country in country_names:\n",
    "    for f in countries_geojson['features']:\n",
    "        if f['properties']['ISO_A3'] == country:\n",
    "            geojson = f\n",
    "            s = shape(f['geometry'])\n",
    "            shapes.append(s)\n",
    "\n",
    "\n",
    "# This may take a while\n",
    "tweets = json.load(open('tweets.json','r'))['tweets']\n",
    "print('Total tweets:',len(tweets))\n",
    "counter = 0\n",
    "t1 = time.time()\n",
    "for tweet in tweets:\n",
    "    \n",
    "    processed_text = re.sub(pattern, ' ', tweet['text'].lower())\n",
    "    processed_text = processed_text.replace('  ',' ').strip()\n",
    "    # sentiment\n",
    "    polarity = TextBlob(processed_text).sentiment.polarity\n",
    "    # remove stopwords and lemmatize\n",
    "    tokens = word_tokenize(processed_text) \n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in ignored_words] \n",
    "    tokens = [t for t in tokens if len(t) > 2 and not t.isnumeric()]\n",
    "    processed_text = ' '.join(t for t in tokens)\n",
    "  \n",
    "    tweet['polarity'] = polarity\n",
    "    tweet['processed_text'] = processed_text\n",
    "    \n",
    "    if 'country' not in tweet:\n",
    "        coords = tweet['coordinates']['coordinates']\n",
    "        tweet['coordinates'] = coords\n",
    "        point = Point(coords)\n",
    "        for i,s in enumerate(shapes):\n",
    "            if s.contains(point):\n",
    "                tweet['country'] = country_names[i]\n",
    "                break\n",
    "                \n",
    "    counter += 1\n",
    "    \n",
    "    if counter % 5000 == 0:\n",
    "        print('Finished',counter,'/',len(tweets),'time elapsed:',round(time.time()-t1,3),'s')\n",
    "        \n",
    "# save to a different file to avoid any errors that might have mutated the original data\n",
    "json.dump({'tweets':tweets},open('tweets2.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: It wasn't necessary to generate word aggregations, as we can do it in real time on the server, it was needed to do the text processing however, as that takes too long for a realtime app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: BEL\n",
      "Time elapsed: 0.03705906867980957\n",
      "\n",
      "Current: NLD\n",
      "Time elapsed: 0.05267906188964844\n",
      "\n",
      "Current: ESP\n",
      "Time elapsed: 0.04389595985412598\n",
      "\n",
      "Current: USA\n",
      "Time elapsed: 0.34644007682800293\n",
      "\n",
      "Current: DEU\n",
      "Time elapsed: 0.05076265335083008\n",
      "\n",
      "Current: CHN\n",
      "Time elapsed: 0.0400388240814209\n",
      "\n",
      "Current: ITA\n",
      "Time elapsed: 0.03120899200439453\n",
      "\n",
      "Current: GBR\n",
      "Time elapsed: 0.1801433563232422\n",
      "\n",
      "Current: IND\n",
      "Time elapsed: 0.10346651077270508\n",
      "\n",
      "Current: SWE\n",
      "Time elapsed: 0.031241893768310547\n",
      "\n",
      "Current: FIN\n",
      "Time elapsed: 0.018566608428955078\n",
      "\n",
      "Current: DNK\n",
      "Time elapsed: 0.02149486541748047\n",
      "\n",
      "Current: PAK\n",
      "Time elapsed: 0.05954432487487793\n",
      "\n",
      "Current: AUS\n",
      "Time elapsed: 0.07219719886779785\n",
      "\n",
      "Current: ROU\n",
      "Time elapsed: 0.014639854431152344\n",
      "\n",
      "Current: HUN\n",
      "Time elapsed: 0.016592025756835938\n",
      "\n",
      "Current: EGY\n",
      "Time elapsed: 0.022448301315307617\n",
      "\n",
      "Current: UKR\n",
      "Time elapsed: 0.015616178512573242\n",
      "\n",
      "Current: IRN\n",
      "Time elapsed: 0.02342510223388672\n",
      "\n",
      "Current: AUT\n",
      "Time elapsed: 0.020496129989624023\n",
      "\n",
      "Current: POL\n",
      "Time elapsed: 0.023448705673217773\n",
      "\n",
      "Current: NZL\n",
      "Time elapsed: 0.030229568481445312\n",
      "\n",
      "Current: NGA\n",
      "Time elapsed: 0.08390974998474121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_tweet(tweet):\n",
    "    polarity = tweet['polarity']\n",
    "    tokens = tweet['processed_text'].split()\n",
    "    # pos or neg\n",
    "    if polarity >= .2:\n",
    "            index = 1\n",
    "    elif polarity <= -.2:\n",
    "        index = 2\n",
    "    else:\n",
    "        index = -1\n",
    "\n",
    "    for t in tokens:\n",
    "        amounts[0] += 1\n",
    "        avg_polarities[0] += polarity\n",
    "        if t in words[0]:\n",
    "            words[0][t] += 1\n",
    "        else:\n",
    "            words[0][t] = 1\n",
    "\n",
    "        if index > 0:\n",
    "            amounts[index] += 1\n",
    "            avg_polarities[index] += polarity\n",
    "            if t in words[index]:\n",
    "                words[index][t] += 1\n",
    "\n",
    "            else:\n",
    "                words[index][t] = 1\n",
    "\n",
    "# Need global vars for easy manipulation\n",
    "# all,pos,neg\n",
    "words = [{},{},{}]\n",
    "avg_polarities = [0,0,0]\n",
    "amounts = [0,0,0]\n",
    "\n",
    "for country,obj in countries.items():\n",
    "    t1 = time.time()\n",
    "    print('Current:',country)\n",
    "    \n",
    "    # Need to redeclare for each country\n",
    "    words = [{},{},{}]\n",
    "    avg_polarities = [0,0,0]\n",
    "    amounts = [0,0,0]\n",
    "\n",
    "    obj_country = json.load(open('tweets_new/tweets_'+country+'.json','r'))\n",
    "    tweets_country = obj_country['tweets']  \n",
    "    \n",
    "    for tweet in tweets_country:\n",
    "        add_tweet(tweet)\n",
    "    # Here, only take the tweets in the respective country\n",
    "    for tweet in tweets:\n",
    "        if 'country' in tweet and tweet['country'] == country:\n",
    "            add_tweet(tweet)\n",
    "            tweets_country.append(tweet)\n",
    "            \n",
    "    avg_polarities = [avg_polarities[i]/amounts[i] for i in range(len(amounts))]\n",
    "     # Get top 100 words for each sentiment\n",
    "    top_words = [dict(sorted(w.items(),reverse=True,key=lambda x:x[1])[:100]) for w in words]\n",
    "    \n",
    "    obj_country['name_iso'] = country\n",
    "    obj_country['name'] = countries[country]['name']\n",
    "    obj_country['words'] = top_words\n",
    "    obj_country['amounts'] = amounts\n",
    "    obj_country['polarity'] = avg_polarities\n",
    "    obj_country['tweets'] = tweets_country\n",
    "        \n",
    "    print('Time elapsed:',time.time()-t1)\n",
    "    print()\n",
    "    # Save in different file just to make sure\n",
    "    json.dump(obj_country,open('tweets_final/tweets_'+country+'.json','w'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
